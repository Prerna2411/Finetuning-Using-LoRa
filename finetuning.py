# -*- coding: utf-8 -*-
"""Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hs63E7weV9_c72jH3lEte-WgaQ-FZQdr
"""

from datasets import load_dataset,DatasetDict,Dataset

from transformers import(
    AutoTokenizer,
    AutoConfig,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer)

from peft import PeftModel,PeftConfig,get_peft_model,LoraConfig
import evaluate
import torch
import numpy as np

!pip install evaluate

###base model

model_checkpoint='distilbert-base-uncased'

##define label maps
id2label={0:"Negative",1:"Positive"}
label2id={"Negative":0,"Positive":1}

##generate classification model fro model_checkpoint
model=AutoModelForSequenceClassification.from_pretrained(
    model_checkpoint,num_labels=2,id2label=id2label,label2id=label2id
)

##load dataset
dataset=load_dataset("shawhin/imdb-truncated")

dataset

pip install -U transformers peft accelerate

###preprocess data

##create tokenizer

tokenizer=AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)


##create tokenize function
def tokenize_function(examples):
    ##extract text
  text=examples["text"]

  ##tokenize  snd truncate text
  tokenizer.truncation_side="left"
  tokenized_inputs=tokenizer(
      text,
      return_tensors="np",
      truncation=True,
      max_length=512
  )
  return tokenized_inputs

##add pad token if none exists
  if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token":"[PAD]"})
    model.resize_token_embeddings(len(tokenizer))

##tokenize training and validation datasets
tokenized_datasets=dataset.map(tokenize_function,batched=True)
tokenized_datasets

##create datacollator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

##evaluation metrics
accuracy=evaluate.load("accuracy")


###define an evaluation function to pass into trainer later
def compute_metrics(p):
  predictions,labels=p
  predictions=np.argmax(predictions,axis=1)

  return accuracy.compute(predictions=predictions,references=labels)

"""Apply untrained model to text


"""

##define list of examples
text_list = ["It was good.", "Not a fan, don't recommed.", "Better than the first one.", "This is not worth watching even once.", "This one is a pass."]

print("Untrained model predictions:")
print("----------------------------")

for text in text_list:
  ##tokenize text
  inputs=tokenizer.encode(text,return_tensors="pt")
  ##compute logits
  logits=model(inputs).logits
  ##convert logits to label
  predictions=torch.argmax(logits)

  print(text + " - " + id2label[predictions.tolist()])

"""Fine Tuning with LoRA"""

peft_config=LoraConfig(task_type="SEQ_CLS",
                       r=4,##intrinsic rank of trainable weights
                       lora_alpha=32,##like learning rate
                       lora_dropout=0.01,##probability of dropout
                       target_modules=['q_lin'])##we apply Lora to the query linear layer

model=get_peft_model(model,peft_config)
model.print_trainable_parameters()

###hyperparameters
lr=2e-5
batch_size=4
num_epochs=10

##define traing arguments
from transformers import EarlyStoppingCallback
training_args=TrainingArguments(
    output_dir=model_checkpoint+"-lora-text-classification",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

###creator trainer object
trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]

)

# train model
trainer.train()

#model.to('cpu') # moving to mps for Mac (can alternatively do 'cpu')

print("Trained model predictions:")
print("--------------------------")
for text in text_list:
    inputs = tokenizer.encode(text, return_tensors="pt") # moving to mps for Mac (can alternatively do 'cpu')
    inputs = inputs.to(model.device) # Move inputs to the same device as the model

    logits = model(inputs).logits
    predictions = torch.max(logits,1).indices

    print(text + " - " + id2label[predictions.tolist()[0]])

# Define the directory to save your model
save_directory = "./my_fine_tuned_model"

# Save the model
trainer.save_model(save_directory)

# Save the tokenizer
tokenizer.save_pretrained(save_directory)

!huggingface-cli login

repo_name = "fine-tuned_distilbert-base-uncased_model"

# Push the model and tokenizer to the Hub
# The commit message and private flag are optional
trainer.push_to_hub(commit_message="Fine-tuned model")



# Commented out IPython magic to ensure Python compatibility.
# %pip install peft

# Commented out IPython magic to ensure Python compatibility.
# %pip install datasets